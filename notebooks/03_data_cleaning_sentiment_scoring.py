# -*- coding: utf-8 -*-
"""03_data_cleaning_sentiment_scoring.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Aib452U58TWdl7LcPEpO_y8CdhpB4Nsc
"""

import pandas as pd
import numpy as np

import tensorflow as tf
print(tf.config.list_physical_devices('GPU'))

df = pd.read_csv("/content/dataset-first.csv")
df.head()

"""Getting Rid OF Outliers"""

df = df[~(df["1d_return"].abs() > 42)]
df = df[~(df["3d_return"].abs() > 42)]
df = df[~(df["5d_return"].abs() > 42)]
df.head()

"""creating a new column which would have highest return value amoung mentioned returns"""

df["max_abs_return"] = df[["1d_return", "3d_return", "5d_return"]].abs().max(axis=1)

df.sort_values(by="max_abs_return", ascending=False).head(5)

print(len(df))

from transformers import pipeline
sentiment = pipeline("sentiment-analysis", model="ahmedrachid/FinancialBERT-Sentiment-Analysis", device=0)
sentiment(["Markets rally after Fed decision", "Company reports unexpected losses"])

df_headlines = df["Headline"]
df_headlines.head()

df_headlines = df_headlines.to_list()
print(df_headlines[:5])

df_headlines_labels = sentiment(df_headlines)

df_headlines_labels_two = df_headlines_labels.copy()

df_top_headlines = df[(df["1d_return"].abs() > 3)] # getting top headlines
df_top_headlines.head()

len(df_top_headlines)

df_headlines_labels_two[-5:]

df_headlines_labels_two = pd.DataFrame(df_headlines_labels_two) # converting the list to DataFrame
df_headlines_labels_two["Headline"] = df["Headline"]
df_headlines_labels_two.head()

df_headlines_labels_two.to_csv("Sentiments.csv",index = False)

print(len(df),len(df_headlines_labels_two))

merged_df = pd.merge(df, df_headlines_labels_two, on='Headline', how='inner') # merging scores and df
merged_df.head()

neutral_mismatches = merged_df[
    (merged_df["label"].str.lower() == "neutral") &
    (merged_df["max_abs_return"].abs() > 3)
]
neutral_mismatches.head() # here stocks showed high returns(-ve or +ve) in a short span but headlines were mentioned neutral,its because the model for sentiment analysis is trained on UK,US stock info not on india,we will be fixing that

len(neutral_mismatches)

neutral_mismatches.sort_values(by="max_abs_return", ascending=False).head(50)

neutral_mismatches.to_csv("neutral_mismatches", index = False)

"""Manual Review of 1000 mismatched sentimens for indian Stcok Market

model will correct the rest after fine tunning
"""

neutral_mismatches = pd.read_csv("/content/neutral_mismatches")
neutral_mismatches=neutral_mismatches.sort_values(by="max_abs_return", ascending=False)
manual_labels = neutral_mismatches.head(1200).copy() # 1200 coz during labeliing I saw many common headlines and we will be droping rows with duplicate headlines

headlines = manual_labels["Headline"].to_list()
len(headlines)

headlines[:5]

correct_sentiments = []
i=0
for i in range(len(headlines)):
  print(headlines[i])
  val = input()
  if val == "q":
    break
  elif val == "n":
    val = "negative"
  elif val == "p":
    val = "positive"
  else:
    val = "neutral"
  correct_sentiments.append(val) # ill not be doing it in 1 sitting,itll take time

manual_labels["Healine"] = headlines
manual_labels["Correct_lables"] = correct_sentiments

manual_labels.to_csv("manual_labels.csv")

manual_labels = pd.read_csv("/content/manual_labels.csv")
len(manual_labels)

manual_labels.head()

manual_labels["Correct_labels"].value_counts()

df_part = neutral_mismatches.head(1200).copy().reset_index(drop=True)
labels = manual_labels["Correct_labels"].reset_index(drop=True)

correct_label_df = pd.concat([df_part, labels], axis=1)

correct_label_df.head()

correct_label_df["Correct_labels"].value_counts()

correct_label_df=correct_label_df.drop_duplicates(subset="Headline") #many headlines were common so we are dropping them,will drop more when entire dataset is complete

len(correct_label_df)

correct_label_df.to_csv("correct_labels.csv",index=False)

correct_label_df_example = correct_label_df.head(50).copy()

correct_label_df_example.to_csv("correct_labels_example.csv",index=False)


